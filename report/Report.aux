\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\HyPL@Entry[1]{}
\abx@aux@refcontext{nyt/global//global/global/global}
\abx@aux@refcontext{nyt/apasortcite//global/global/global}
\abx@aux@cite{0}{rajkomar2019}
\abx@aux@segm{0}{0}{rajkomar2019}
\abx@aux@refcontext{nyt/apasortcite//global/global/global}
\abx@aux@cite{0}{johnson2016}
\abx@aux@segm{0}{0}{johnson2016}
\HyPL@Entry{0<</S/D>>}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\abx@aux@refcontext{nyt/apasortcite//global/global/global}
\abx@aux@cite{0}{johnson2023mimiciv}
\abx@aux@segm{0}{0}{johnson2023mimiciv}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces 긴장성 기흉 예시}}{2}{figure.1}\protected@file@percent }
\newlabel{fig:pneumothorax}{{1}{2}{긴장성 기흉 예시}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Method}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Data Collection and Cohort Definition}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Data Sources}{2}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Cohort Selection Criteria}{2}{subsubsection.2.1.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Cohort Definition and Selection Criteria}}{3}{table.1}\protected@file@percent }
\newlabel{tab:cohort}{{1}{3}{Cohort Definition and Selection Criteria}{table.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Reference Time Definition}{3}{subsubsection.2.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Biological Signal and Clinical Variable Extraction}{3}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Target Variables}{3}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Extracted Biological Signals and Clinical Variables}}{3}{table.2}\protected@file@percent }
\newlabel{tab:variables}{{2}{3}{Extracted Biological Signals and Clinical Variables}{table.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Time Window}{4}{subsubsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Feature Engineering}{4}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Domain Knowledge-based Derived Variables}{4}{subsubsection.2.3.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Domain Knowledge-based Derived Variables}}{4}{table.3}\protected@file@percent }
\newlabel{tab:derived}{{3}{4}{Domain Knowledge-based Derived Variables}{table.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Time Series Feature Analysis}{4}{subsubsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Multiple Time Window}{5}{subsubsection.2.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Data Preprocessing}{5}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}Missing Value Weight based Sample Removal for Data Quality Assurance}{5}{subsubsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Greedy Sample Removal}{5}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Removal Priority Score}{5}{section*.2}\protected@file@percent }
\abx@aux@refcontext{nyt/apasortcite//global/global/global}
\abx@aux@cite{0}{buuren2011}
\abx@aux@segm{0}{0}{buuren2011}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}Asymmetric Threshold-based Feature Selection}{6}{subsubsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.3}Multiple Imputation for Missing Values}{6}{subsubsection.2.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.4}Normalization}{6}{subsubsection.2.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.5}Final Dataset Construction}{6}{subsubsection.2.4.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Number of Subjects at Each Preprocessing Step}}{6}{table.4}\protected@file@percent }
\newlabel{tab:cohort_counts}{{4}{6}{Number of Subjects at Each Preprocessing Step}{table.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Machine Learning Pipeline}{6}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1}Feature Selection}{6}{subsubsection.2.5.1}\protected@file@percent }
\abx@aux@refcontext{nyt/apasortcite//global/global/global}
\abx@aux@cite{0}{lin2017}
\abx@aux@segm{0}{0}{lin2017}
\abx@aux@refcontext{nyt/apasortcite//global/global/global}
\abx@aux@cite{0}{akiba2019}
\abx@aux@segm{0}{0}{akiba2019}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.2}Cost-sensitive Learning for Class Imbalance}{7}{subsubsection.2.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.3}Cross-validation}{7}{subsubsection.2.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.4}Baseline Model}{7}{subsubsection.2.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.5}Model Selection and Hyperparameter Optimization}{7}{subsubsection.2.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.6}Threshold Optimization}{8}{subsubsection.2.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.7}Ensemble Strategy}{8}{subsubsection.2.5.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{8}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Optimal Hyperparameters}{8}{subsection.3.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Optimal Hyperparameters Results}}{8}{table.5}\protected@file@percent }
\newlabel{tab:hyperparams}{{5}{8}{Optimal Hyperparameters Results}{table.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Ablation Study on Preprocessing Strategies}{9}{subsection.3.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Ablation Study of Preprocessing Strategies}}{9}{table.6}\protected@file@percent }
\newlabel{tab:ablation}{{6}{9}{Ablation Study of Preprocessing Strategies}{table.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Model Performance Evaluation}{9}{subsection.3.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Model Performance Comparison (Mean $\pm $ SD)}}{10}{table.7}\protected@file@percent }
\newlabel{tab:results}{{7}{10}{Model Performance Comparison (Mean $\pm $ SD)}{table.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Confusion Matrix (XGBoost). TP: 109, FN: 41, FP: 52, TN: 698.}}{10}{figure.2}\protected@file@percent }
\newlabel{fig:confusion_matrix}{{2}{10}{Confusion Matrix (XGBoost). TP: 109, FN: 41, FP: 52, TN: 698}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Statistical Analysis and Clinical Utility}{10}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Statistical Significance Test}{10}{subsubsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Calibration Analysis}{10}{subsubsection.3.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Calibration curves comparing XGBoost and LightGBM. The XGBoost model (Brier: 0.142) demonstrates better calibration compared to LightGBM (Brier: 0.213), which tends to underestimate the risk probabilities.}}{11}{figure.3}\protected@file@percent }
\newlabel{fig:calibration}{{3}{11}{Calibration curves comparing XGBoost and LightGBM. The XGBoost model (Brier: 0.142) demonstrates better calibration compared to LightGBM (Brier: 0.213), which tends to underestimate the risk probabilities}{figure.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.3}Decision Curve Analysis}{11}{subsubsection.3.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Decision Curve Analysis (DCA). The y-axis measures the net benefit. The XGBoost model shows a positive net benefit in the low threshold probability range ($0.0 - 0.2$), suggesting its utility as a screening tool. In contrast, LightGBM fails to provide clinical utility across relevant thresholds.}}{12}{figure.4}\protected@file@percent }
\newlabel{fig:dca}{{4}{12}{Decision Curve Analysis (DCA). The y-axis measures the net benefit. The XGBoost model shows a positive net benefit in the low threshold probability range ($0.0 - 0.2$), suggesting its utility as a screening tool. In contrast, LightGBM fails to provide clinical utility across relevant thresholds}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Key Predictive Features}{12}{subsection.3.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Key Predictive Features (XGBoost)}}{13}{table.8}\protected@file@percent }
\newlabel{tab:features}{{8}{13}{Key Predictive Features (XGBoost)}{table.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Top 5 features ranked by permutation importance in (A) XGBoost and (B) LightGBM. RR: respiratory rate; MAD: median absolute deviation; DBP: diastolic blood pressure; MAP: mean arterial pressure; RMSSD: root mean square of successive differences.}}{13}{figure.5}\protected@file@percent }
\newlabel{fig:feature_importance}{{5}{13}{Top 5 features ranked by permutation importance in (A) XGBoost and (B) LightGBM. RR: respiratory rate; MAD: median absolute deviation; DBP: diastolic blood pressure; MAP: mean arterial pressure; RMSSD: root mean square of successive differences}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}SHAP Analysis for Model Interpretability and Local Interpretability}{14}{subsection.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.1}Global Interpretability and Feature Impact}{14}{subsubsection.3.6.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces SHAP beeswarm plot summarizing the impact of features on model output. Red dots represent high feature values, blue dots represent low values. Positive SHAP values indicate a higher predicted risk of tension pneumothorax.}}{14}{figure.6}\protected@file@percent }
\newlabel{fig:shap_beeswarm}{{6}{14}{SHAP beeswarm plot summarizing the impact of features on model output. Red dots represent high feature values, blue dots represent low values. Positive SHAP values indicate a higher predicted risk of tension pneumothorax}{figure.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.2}Feature Dependence and Non-linearity Analysis}{14}{subsubsection.3.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces SHAP dependence plots for the top 6 features. These plots reveal non-linear relationships between feature values (x-axis) and their impact on the model's prediction (y-axis).}}{15}{figure.7}\protected@file@percent }
\newlabel{fig:shap_dependence}{{7}{15}{SHAP dependence plots for the top 6 features. These plots reveal non-linear relationships between feature values (x-axis) and their impact on the model's prediction (y-axis)}{figure.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.6.3}Local Interpretability analysis using SHAP Force Plots}{15}{subsubsection.3.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Case 1: True Positive}{15}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Case 2: False Positive}{16}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Case 3: False Negative}{16}{section*.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces SHAP force plots for individual predictions. (Top) True Positive case where the model correctly identified high risk. (Bottom) False Positive case illustrating potential false alarms. Red bars indicate features pushing the prediction towards positive (risk), while blue bars push towards negative (safe).}}{17}{figure.8}\protected@file@percent }
\newlabel{fig:shap_force}{{8}{17}{SHAP force plots for individual predictions. (Top) True Positive case where the model correctly identified high risk. (Bottom) False Positive case illustrating potential false alarms. Red bars indicate features pushing the prediction towards positive (risk), while blue bars push towards negative (safe)}{figure.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Error Analysis}{17}{subsection.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.7.1}False Negative Analysis}{17}{subsubsection.3.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Distribution of top distinguishing features for False Negative cases in XGBoost. The presence of features with kurtosis values of -1.5 suggests that signal loss or artificial flatness due to imputation contributed to missed detections.}}{18}{figure.9}\protected@file@percent }
\newlabel{fig:error_fn}{{9}{18}{Distribution of top distinguishing features for False Negative cases in XGBoost. The presence of features with kurtosis values of -1.5 suggests that signal loss or artificial flatness due to imputation contributed to missed detections}{figure.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.7.2}False Positive Analysis}{18}{subsubsection.3.7.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Distribution of top distinguishing features for False Positive cases in XGBoost. Features with zero sample entropy or extreme kurtosis indicate that the model may misinterpret signal artifacts or flatlines as pathological anomalies.}}{18}{figure.10}\protected@file@percent }
\newlabel{fig:error_fp}{{10}{18}{Distribution of top distinguishing features for False Positive cases in XGBoost. Features with zero sample entropy or extreme kurtosis indicate that the model may misinterpret signal artifacts or flatlines as pathological anomalies}{figure.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{19}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Interpretation}{19}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Methodological Strengths}{19}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Weight-based Sample Removal Algorithm}{19}{subsubsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Asymmetric Threshold-based Feature Selection}{20}{subsubsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}MICE}{20}{subsubsection.4.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4}Cost-sensitive Learning and Focal Loss}{20}{subsubsection.4.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.5}GroupKFold Cross-validation}{20}{subsubsection.4.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Clinical Implications}{20}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Limitations}{21}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contents